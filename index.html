<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>ZBra Dataset</title>
    <link rel="stylesheet" href="static/style.css">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>
<body>
    <!-- 顶部 Logo -->
    <header class="logo-container">
        <img src="static/logo.png" alt="Logo" class="logo-img">
    </header>

    <!-- 导航栏 -->
    <nav class="navbar">
        <ul>
            <li><a href="index.html" class="active">HOME</a></li>
            <li><a href="description.html">DESCRIPTION</a></li>
            <li><a href="download.html">DOWNLOAD</a></li>
            <li><a href="contact.html">CONTACT US</a></li>
        </ul>
    </nav>

    <!-- 主体内容 -->
    <main class="content">
        <p>ZJU Brain Dataset (ZBra) is a collection of multimodal brain signal datasets (e.g., EEG, fNIRS)  provided by the State Key Laboratory of Brain-machine Intelligence, Zhejiang University. We will provide a series of brain datasets for the academic community to explore. The datasets are managed by <a href="http://www.shazhao.net" target="_blank" rel="noopener noreferrer">Dr. Sha Zhao</a> and <a href="https://person.zju.edu.cn/en/gpan" target="_blank" rel="noopener noreferrer">Prof. Gang Pan</a>. 
        </p>

        <p> The first dataset we provide is <span style="font-weight: bold; font-size: 1.1em;">ZBra-music Dataset</span>, which can be used for emotion analysis. It consists of EEG and fNIRS signals recorded when subjetcts are listening to emotion relatd music clips that are AI-generated. 
        
        <h1><a href="description.html" style="color: black;">ZBra-music Dataset</a></h1>

        <p>
            The dataset comprises physiological recordings (EEG and fNIRS) collected from <span style="font-weight: bold; font-size: 1.1em;">44 participants</span> via a wireless headband as they listened to music generated by AI, 
            with a total duration of <span style="font-weight: bold; font-size: 1.1em;">~1,720 minutes</span>. 
            Each music clip is associated with one of four emotional states representing the valence-arousal quadrants: 
            high arousal-high valence (HAHV), high arousal-low valence (HALV), low arousal-high valence (LAHV), and low arousal-low valence (LALV).
            EEG data is recorded from two prefrontal channels (Fp1, Fp2) using a portable wireless headset. 
            fNIRS data is collected from eight optodes to measure prefrontal hemodynamic activity. 
        </p>

        <img src="static/ZBra_music_overview.png" alt="MEEtBrain" class="contain-img">
        Click <a href="description.html">here</a> to know more details about the dataset.

        <h2>Paper</h2>
        <p>[1] Sha Zhao, Song Yi, Yangxuan Zhou, Jiadong Pan, Jiquan Wang, Jie Xia, Shijian Li, Shurong Dong, and Gang Pan. 2025. Wearable Music2Emotion : Assessing Emotions Induced by AI-Generated Music through Portable 
            EEG-fNIRS Fusion. In <em>Proceedings of the 33rd ACM International Conference 
            on Multimedia (MM '25), October 27-31, 2025, Dublin, Ireland</em>. https://doi.org/10.1145/3746027.3755270</p>
    </main>
</body>
</html>
